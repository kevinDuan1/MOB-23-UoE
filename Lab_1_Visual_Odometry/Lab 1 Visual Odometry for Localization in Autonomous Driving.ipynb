{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visual Odometry for Localization in Autonomous Driving\n",
    "\n",
    "Welcome to the assignment for lab 1: Visual Features - Detection, Description and Matching. In this assignment, you will practice using the material you have learned to estimate an autonomous vehicle trajectory by images taken with a monocular camera set up on the vehicle.\n",
    "\n",
    "<img src=\"./slide/1.png\">\n",
    "<img src=\"./slide/2.png\">\n",
    "\n",
    "\n",
    "**In this assignment, you will:**\n",
    "- Extract  features from the photographs taken with a camera setup on the vehicle.\n",
    "- Use the extracted features to find matches between the features in different photographs.\n",
    "- Learn how to use the found matches to estimate the camera motion between subsequent photographs. \n",
    "- Learn how to use the estimated camera motion to build the vehicle trajectory.\n",
    "\n",
    "For most exercises, you are provided with a suggested outline. You are encouraged to diverge from the outline if you think there is a better, more efficient way to solve a problem.\n",
    "\n",
    "You are only allowed to use the packages loaded bellow and the custom functions explained in the notebook. Run the cell bellow to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from m2bk import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Loading and Visualizing the Data\n",
    "We provide you with a convenient dataset handler class to read and iterate through samples taken from the CARLA simulator. Run the following code to create a dataset handler object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_handler = DatasetHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset handler contains 100 data frames. Each frame contains an RGB image and a depth map taken with a setup on the vehicle and a grayscale version of the RGB image which will be used for computation. Furthermore, camera calibration matrix K is also provided in the dataset handler.\n",
    "\n",
    "Upon creation of the dataset handler object, all the frames will be automatically read and loaded. The frame content can be accessed by using `images`, `images_rgb`, `depth_maps` attributes of the dataset handler object along with the index of the requested frame. See how to access the images (grayscale), rgb images (3-channel color), depth maps and camera calibration matrix in the example below.\n",
    "\n",
    "**Note (Depth Maps)**: Maximum depth distance is 1000. This value of depth shows that the selected pixel is at least 1000m (1km) far from the camera, however the exact distance of this pixel from the camera is unknown. Having this kind of points in further trajectory estimation might affect the trajectory precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset_handler.images[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_rgb = dataset_handler.images_rgb[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "depth = dataset_handler.depth_maps[i]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(depth, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Depth map shape: {0}\".format(depth.shape))\n",
    "\n",
    "v, u = depth.shape\n",
    "depth_val = depth[v-1, u-1]\n",
    "print(\"Depth value of the very bottom-right pixel of depth map {0} is {1:0.3f}\".format(i, depth_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_handler.k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access an arbitrary frame use image index, as shown in the examples below. Make sure the indexes are within the number of frames in the dataset. The number of frames in the dataset can be accessed with num_frames attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of frames in the dataset\n",
    "print(dataset_handler.num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 30\n",
    "image = dataset_handler.images[i]\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Feature Extraction\n",
    "\n",
    "### 1.1 - Extracting Features from an Image\n",
    "\n",
    "**Task**: Implement feature extraction from a single image. You can use any feature descriptor of your choice covered in the lectures, ORB for example. \n",
    "\n",
    "\n",
    "Note 1: Make sure you understand the structure of the keypoint descriptor object, this will be very useful for your further tasks.\n",
    "\n",
    "Note 2: Make sure you understand the image coordinate system, namely the origin location and axis directions.\n",
    "\n",
    "Note 3: We provide you with a function to visualise the features detected. Run the last 2 cells in section 1.1 to view.\n",
    "\n",
    "\n",
    "***Optional***: Try to extract features with different descriptors such as SIFT (cv2.SIFT_create()),  ORB (cv2.ORB_create()), SURF and BRIEF. You can also try using detectors such as Harris corners or FAST and pairing them with a descriptor. Lastly, try changing parameters of the algorithms. Do you see the difference in various approaches?\n",
    "You might find this link useful:  [OpenCV:Feature Detection and Description](https://docs.opencv.org/3.4.3/db/d27/tutorial_py_table_of_contents_feature2d.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Find keypoints and descriptors for the image\n",
    "\n",
    "    Arguments:\n",
    "    image -- a grayscale image\n",
    "\n",
    "    Returns:\n",
    "    kp -- list of the extracted keypoints (features) in an image\n",
    "    des -- list of the keypoint descriptors in an image\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image = dataset_handler.images[i]\n",
    "kp, des = extract_features(image)\n",
    "print(\"Number of features detected in frame {0}: {1}\\n\".format(i, len(kp)))\n",
    "\n",
    "print(\"Coordinates of the first keypoint in frame {0}: {1}\".format(i, str(kp[0].pt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(image, kp):\n",
    "    \"\"\"\n",
    "    Visualize extracted features in the image\n",
    "\n",
    "    Arguments:\n",
    "    image -- a grayscale image\n",
    "    kp -- list of the extracted keypoints\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    display = cv2.drawKeypoints(image, kp, None)\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.imshow(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: visualizing and experimenting with various feature descriptors\n",
    "i = 0\n",
    "image = dataset_handler.images_rgb[i]\n",
    "\n",
    "visualize_features(image, kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_2(image):\n",
    "    \"\"\"\n",
    "    Find keypoints and descriptors for the image\n",
    "\n",
    "    Arguments:\n",
    "    image -- a grayscale image\n",
    "\n",
    "    Returns:\n",
    "    kp -- list of the extracted keypoints (features) in an image\n",
    "    des -- list of the keypoint descriptors in an image\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return kp, des\n",
    "\n",
    "kp2,_ = extract_features_2(image)\n",
    "visualize_features(image, kp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Extracting Features from Each Image in the Dataset\n",
    "\n",
    "**Task**: Implement feature extraction for each image in the dataset with the function you wrote in the above section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dataset(images, extract_features_function):\n",
    "    \"\"\"\n",
    "    Find keypoints and descriptors for each image in the dataset\n",
    "\n",
    "    Arguments:\n",
    "    images -- a list of grayscale images\n",
    "    extract_features_function -- a function which finds features (keypoints and descriptors) for an image\n",
    "\n",
    "    Returns:\n",
    "    kp_list -- a list of keypoints for each image in images\n",
    "    des_list -- a list of descriptors for each image in images\n",
    "    \n",
    "    \"\"\"\n",
    "    kp_list = []\n",
    "    des_list = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return kp_list, des_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset_handler.images\n",
    "kp_list, des_list = extract_features_dataset(images, extract_features)\n",
    "\n",
    "i = 0\n",
    "print(\"Number of features detected in frame {0}: {1}\".format(i, len(kp_list[i])))\n",
    "print(\"Coordinates of the first keypoint in frame {0}: {1}\\n\".format(i, str(kp_list[i][0].pt)))\n",
    "\n",
    "# Remember that the length of the returned by dataset_handler lists should be the same as the length of the image array\n",
    "print(\"Length of images array: {0}\".format(len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Feature Matching\n",
    "\n",
    "Next step after extracting the features in each image is matching the features from the subsequent frames. This is what is needed to be done in this section.\n",
    "\n",
    "### 2.1 - Matching Features from a Pair of Subsequent Frames\n",
    "\n",
    "**Task**: Implement feature matching for a pair of images. You can use any feature matching algorithm of your choice covered in the lectures, Brute Force Matching or FLANN based Matching for example. \n",
    "\n",
    "**Note**: you might find these APIs useful:  cv2.BFMatcher() and cv2.FlannBasedMatcher().\n",
    "https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n",
    "\n",
    "***Optional 1***: Implement match filtering by thresholding the distance between the best matches (Lowe's ration test). This might be useful for improving your overall trajectory estimation results. Recall that you have an option of specifying the number best matches to be returned by the knnmatcher.\n",
    "\n",
    "We have provided a visualization of the found matches. Do all the matches look legitimate to you? Do you think match filtering can improve the situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(des1, des2):\n",
    "    \"\"\"\n",
    "    Match features from two images\n",
    "\n",
    "    Arguments:\n",
    "    des1 -- list of the keypoint descriptors in the first image\n",
    "    des2 -- list of the keypoint descriptors in the second image\n",
    "\n",
    "    Returns:\n",
    "    match -- list of matched features from two images. Each match[i] is k or less matches for the same query descriptor\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "des1 = des_list[i]\n",
    "des2 = des_list[i+1]\n",
    "\n",
    "match = match_features(des1, des2)\n",
    "print(\"Number of features matched in frames {0} and {1}: {2}\".format(i, i+1, len(match)))\n",
    "\n",
    "# Remember that a matcher finds the best matches for EACH descriptor from a query set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "def filter_matches_distance(match, dist_threshold):\n",
    "    \"\"\"\n",
    "    Filter matched features from two images by distance between the best matches\n",
    "\n",
    "    Arguments:\n",
    "    match -- list of matched features from two images\n",
    "    dist_threshold -- maximum allowed relative distance between the best matches, (0.0, 1.0) \n",
    "\n",
    "    Returns:\n",
    "    filtered_match -- list of good matches, satisfying the distance threshold\n",
    "    \"\"\"\n",
    "    filtered_match = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return filtered_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "i = 0 \n",
    "des1 = des_list[i]\n",
    "des2 = des_list[i+1]\n",
    "match = match_features(des1, des2)\n",
    "\n",
    "dist_threshold = 0.6\n",
    "filtered_match = filter_matches_distance(match, dist_threshold)\n",
    "\n",
    "print(\"Number of features matched in frames {0} and {1} after filtering by distance: {2}\".format(i, i+1, len(filtered_match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(image1, kp1, image2, kp2, match):\n",
    "    \"\"\"\n",
    "    Visualize corresponding matches in two images\n",
    "\n",
    "    Arguments:\n",
    "    image1 -- the first image in a matched image pair\n",
    "    kp1 -- list of the keypoints in the first image\n",
    "    image2 -- the second image in a matched image pair\n",
    "    kp2 -- list of the keypoints in the second image\n",
    "    match -- list of matched features from the pair of images\n",
    "\n",
    "    Returns:\n",
    "    image_matches -- an image showing the corresponding matches on both image1 and image2 or None if you don't use this function\n",
    "    \"\"\"\n",
    "    image_matches = cv2.drawMatches(image1,kp1,image2,kp2,match,None)\n",
    "    plt.figure(figsize=(16, 6), dpi=100)\n",
    "    plt.imshow(image_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize n first matches, set n to None to view all matches\n",
    "# set filtering to True if using match filtering, otherwise set to False\n",
    "n = None\n",
    "filtering = True\n",
    "\n",
    "i = 0 \n",
    "image1 = dataset_handler.images[i]\n",
    "image2 = dataset_handler.images[i+1]\n",
    "\n",
    "kp1 = kp_list[i]\n",
    "kp2 = kp_list[i+1]\n",
    "\n",
    "des1 = des_list[i]\n",
    "des2 = des_list[i+1]\n",
    "\n",
    "match = match_features(des1, des2)\n",
    "if filtering:\n",
    "    dist_threshold = 0.6\n",
    "    match = filter_matches_distance(match, dist_threshold)\n",
    "\n",
    "image_matches = visualize_matches(image1, kp1, image2, kp2, match[:n])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Matching Features in Each Subsequent Image Pair in the Dataset\n",
    "\n",
    "**Task**: Implement feature matching for each subsequent image pair in the dataset with the function you wrote in the above section.\n",
    "\n",
    "***Optional***: Implement match filtering by thresholding the distance for each subsequent image pair in the dataset with the function you wrote in the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features_dataset(des_list, match_features):\n",
    "    \"\"\"\n",
    "    Match features for each subsequent image pair in the dataset\n",
    "\n",
    "    Arguments:\n",
    "    des_list -- a list of descriptors for each image in the dataset\n",
    "    match_features -- a function which maches features between a pair of images\n",
    "\n",
    "    Returns:\n",
    "    matches -- list of matches for each subsequent image pair in the dataset. \n",
    "               Each matches[i] is a list of matched features from images i and i + 1\n",
    "               \n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = match_features_dataset(des_list, match_features)\n",
    "\n",
    "i = 0\n",
    "print(\"Number of features matched in frames {0} and {1}: {2}\".format(i, i+1, len(matches[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "def filter_matches_dataset(filter_matches_distance, matches, dist_threshold):\n",
    "    \"\"\"\n",
    "    Filter matched features by distance for each subsequent image pair in the dataset\n",
    "\n",
    "    Arguments:\n",
    "    filter_matches_distance -- a function which filters matched features from two images by distance between the best matches\n",
    "    matches -- list of matches for each subsequent image pair in the dataset. \n",
    "               Each matches[i] is a list of matched features from images i and i + 1\n",
    "    dist_threshold -- maximum allowed relative distance between the best matches, (0.0, 1.0) \n",
    "\n",
    "    Returns:\n",
    "    filtered_matches -- list of good matches for each subsequent image pair in the dataset. \n",
    "                        Each matches[i] is a list of good matches, satisfying the distance threshold\n",
    "               \n",
    "    \"\"\"\n",
    "    filtered_matches = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return filtered_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "dist_threshold = 0.6\n",
    "\n",
    "filtered_matches = filter_matches_dataset(filter_matches_distance, matches, dist_threshold)\n",
    "\n",
    "if len(filtered_matches) > 0:\n",
    "    \n",
    "    # Make sure that this variable is set to True if you want to use filtered matches further in your assignment\n",
    "    is_main_filtered_m = ...\n",
    "    if is_main_filtered_m: \n",
    "        matches = filtered_matches\n",
    "\n",
    "    i = 0\n",
    "    print(\"Number of filtered matches in frames {0} and {1}: {2}\".format(i, i+1, len(filtered_matches[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Trajectory Estimation (Optional)\n",
    "\n",
    "At this point you have everything to perform visual odometry for the autonomous vehicle. In this section you will incrementally estimate the pose of the vehicle by examining the changes that motion induces on the images of its onboard camera.\n",
    "\n",
    "### 3.1 - Estimating Camera Motion between a Pair of Images\n",
    "\n",
    "Camera motion estimation from a pair of images. You can check the motion estimation algorithm namely Perspective-n-Point (PnP), as well as Essential Matrix Decomposition.\n",
    "\n",
    "- We have provided code for PnP, if you decide to use Essential Matrix Decomposition, more information about this method can be found in [Wikipedia: Determining R and t from E](https://en.wikipedia.org/wiki/Essential_matrix).\n",
    "\n",
    "You don't need to understand these algorithms themselves, but you should know how to use these python APIs.\n",
    "\n",
    "More information on both approaches implementation can be found in [OpenCV: Camera Calibration and 3D Reconstruction](https://docs.opencv.org/3.4.3/d9/d0c/group__calib3d.html). Specifically, you might be interested in _Detailed Description_ section of [OpenCV: Camera Calibration and 3D Reconstruction](https://docs.opencv.org/3.4.3/d9/d0c/group__calib3d.html) as it explains the connection between the 3D world coordinate system and the 2D image coordinate system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_motion(match, kp1, kp2, k, depth1=None):\n",
    "    \"\"\"\n",
    "    Estimate camera motion from a pair of subsequent image frames\n",
    "\n",
    "    Arguments:\n",
    "    match -- list of matched features from the pair of images\n",
    "    kp1 -- list of the keypoints in the first image\n",
    "    kp2 -- list of the keypoints in the second image\n",
    "    k -- camera calibration matrix \n",
    "    \n",
    "    Optional arguments:\n",
    "    depth1 -- a depth map of the first frame. This argument is not needed if you use Essential Matrix Decomposition\n",
    "\n",
    "    Returns:\n",
    "    rmat -- recovered 3x3 rotation numpy matrix\n",
    "    tvec -- recovered 3x1 translation numpy vector\n",
    "    image1_points -- a list of selected match coordinates in the first image. image1_points[i] = [u, v], where u and v are \n",
    "                     coordinates of the i-th match in the image coordinate system\n",
    "    image2_points -- a list of selected match coordinates in the second image. image1_points[i] = [u, v], where u and v are \n",
    "                     coordinates of the i-th match in the image coordinate system\n",
    "               \n",
    "    \"\"\"\n",
    "    rmat = np.eye(3)\n",
    "    tvec = np.zeros((3, 1))\n",
    "    image1_points = []\n",
    "    image2_points = []\n",
    "    \n",
    "\n",
    "    objectpoints = []\n",
    "    for m in match:\n",
    "        # Coordinates of m\n",
    "        u1, v1 = kp1[m.queryIdx].pt\n",
    "        u2, v2 = kp2[m.trainIdx].pt\n",
    "        \n",
    "        # Get depth\n",
    "        s = depth1[int(v1), int(u1)]\n",
    "        \n",
    "        # Check for valid scale values\n",
    "        if s < 1000:\n",
    "            # Transform pixel coordinates to camera coordinates using the pinhole camera model\n",
    "            p_c = np.linalg.inv(k) @ (s * np.array([u1, v1, 1]))\n",
    "            \n",
    "            # Save the results\n",
    "            image1_points.append([u1, v1])\n",
    "            image2_points.append([u2, v2])\n",
    "            objectpoints.append(p_c)\n",
    "        \n",
    "    # Convert lists to numpy arrays\n",
    "    objectpoints = np.vstack(objectpoints)\n",
    "    imagepoints = np.array(image2_points)\n",
    "    \n",
    "    # Determine the camera pose from the Perspective-n-Point solution using the RANSAC scheme\n",
    "    _, rvec, tvec, _ = cv2.solvePnPRansac(objectpoints, imagepoints, k, None)\n",
    "    # Convert rotation vector to rotation matrix\n",
    " \n",
    "    rmat, _ = cv2.Rodrigues(rvec)\n",
    "    \n",
    "\n",
    "    \n",
    "    return rmat, tvec, image1_points, image2_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "match = matches[i]\n",
    "kp1 = kp_list[i]\n",
    "kp2 = kp_list[i+1]\n",
    "k = dataset_handler.k\n",
    "depth = dataset_handler.depth_maps[i]\n",
    "\n",
    "rmat, tvec, image1_points, image2_points = estimate_motion(match, kp1, kp2, k, depth1=depth)\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Camera Movement Visualization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=30\n",
    "image1  = dataset_handler.images_rgb[i]\n",
    "image2 = dataset_handler.images_rgb[i + 1]\n",
    "\n",
    "image_move = visualize_camera_movement(image1, image1_points, image2, image2_points)\n",
    "plt.figure(figsize=(16, 12), dpi=100)\n",
    "plt.imshow(image_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_move = visualize_camera_movement(image1, image1_points, image2, image2_points, is_show_img_after_move=True)\n",
    "plt.figure(figsize=(16, 12), dpi=100)\n",
    "plt.imshow(image_move)\n",
    "# These visualizations might be helpful for understanding the quality of image points selected for the camera motion estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Camera Trajectory Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_trajectory(estimate_motion, matches, kp_list, k, depth_maps=[], save=''):\n",
    "    \"\"\"\n",
    "    Estimate complete camera trajectory from subsequent image pairs\n",
    "\n",
    "    Arguments:\n",
    "    estimate_motion -- a function which estimates camera motion from a pair of subsequent image frames\n",
    "    matches -- list of matches for each subsequent image pair in the dataset. \n",
    "               Each matches[i] is a list of matched features from images i and i + 1\n",
    "    des_list -- a list of keypoints for each image in the dataset\n",
    "    k -- camera calibration matrix \n",
    "    \n",
    "    Optional arguments:\n",
    "    depth_maps -- a list of depth maps for each frame. This argument is not needed if you use Essential Matrix Decomposition\n",
    "    save -- a path to store camera movement images, it will not save images by default\n",
    "\n",
    "    Returns:\n",
    "    trajectory -- a 3xlen numpy array of the camera locations, where len is the lenght of the list of images and   \n",
    "                  trajectory[:, i] is a 3x1 numpy vector, such as:\n",
    "                  \n",
    "                  trajectory[:, i][0] - is X coordinate of the i-th location\n",
    "                  trajectory[:, i][1] - is Y coordinate of the i-th location\n",
    "                  trajectory[:, i][2] - is Z coordinate of the i-th location\n",
    "                  \n",
    "                  * Consider that the origin of your trajectory cordinate system is located at the camera position \n",
    "                  when the first image (the one with index 0) was taken. The first camera location (index = 0) is geven \n",
    "                  at the initialization of this function\n",
    "\n",
    "    \"\"\"        \n",
    "    # Create variables for computation\n",
    "    trajectory = np.zeros((3, len(matches) + 1))\n",
    "    robot_pose = np.zeros((len(matches) + 1, 4, 4))\n",
    "    \n",
    "    # Initialize camera pose\n",
    "    robot_pose[0] = np.eye(4)\n",
    "    \n",
    "    # Iterate through the matched features\n",
    "    for i in range(len(matches)):\n",
    "        # Estimate camera motion between a pair of images\n",
    "        rmat, tvec, image1_points, image2_points = estimate_motion(matches[i], kp_list[i], kp_list[i + 1], k, depth_maps[i])\n",
    "        \n",
    "        # Save camera movement visualization\n",
    "        if save:\n",
    "            image = visualize_camera_movement(dataset_handler.images_rgb[i], image1_points, dataset_handler.images_rgb[i + 1], image2_points)\n",
    "            plt.imsave('{}/frame_{:02d}.jpg'.format(save, i), image)\n",
    "        \n",
    "        # Determine current pose from rotation and translation matrices\n",
    "        current_pose = np.eye(4)\n",
    "        current_pose[0:3, 0:3] = rmat\n",
    "        current_pose[0:3, 3] = tvec.T\n",
    "        \n",
    "        # Build the robot's pose from the initial position by multiplying previous and current poses\n",
    "        robot_pose[i + 1] = robot_pose[i] @ np.linalg.inv(current_pose)\n",
    "        \n",
    "        # Calculate current camera position from origin\n",
    "        position = robot_pose[i + 1] @ np.array([0., 0., 0., 1.])\n",
    "        \n",
    "        # Build trajectory\n",
    "        trajectory[:, i + 1] = position[0:3]\n",
    "        \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "depth_maps = dataset_handler.depth_maps\n",
    "trajectory = estimate_trajectory(estimate_motion, matches, kp_list, k, depth_maps=depth_maps)\n",
    "\n",
    "i = 1\n",
    "print(\"Camera location in point {0} is: \\n {1}\\n\".format(i, trajectory[:, [i]]))\n",
    "\n",
    "# Remember that the length of the returned by trajectory should be the same as the length of the image array\n",
    "print(\"Length of trajectory: {0}\".format(trajectory.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "Camera location in point i is: \n",
    " [[locXi]\n",
    " [locYi]\n",
    " [locZi]]```\n",
    " \n",
    " In this output: locXi, locYi, locZi are the coordinates of the corresponding i-th camera location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: Make sure to uncomment the below line if you modified the original data in any ways\n",
    "#dataset_handler = DatasetHandler()\n",
    "\n",
    "\n",
    "# Part 1. Features Extraction\n",
    "images = dataset_handler.images\n",
    "kp_list, des_list = extract_features_dataset(images, extract_features)\n",
    "\n",
    "\n",
    "# Part II. Feature Matching\n",
    "matches = match_features_dataset(des_list, match_features)\n",
    "\n",
    "# Set to True if you want to use filtered matches or False otherwise\n",
    "is_main_filtered_m = True\n",
    "if is_main_filtered_m:\n",
    "    dist_threshold = 0.75\n",
    "    filtered_matches = filter_matches_dataset(filter_matches_distance, matches, dist_threshold)\n",
    "    matches = filtered_matches\n",
    "\n",
    "    \n",
    "# Part III. Trajectory Estimation\n",
    "depth_maps = dataset_handler.depth_maps\n",
    "trajectory = estimate_trajectory(estimate_motion, matches, kp_list, k, depth_maps=depth_maps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your Results\n",
    "\n",
    " Assure that your trajectory axis directions follow the ones in _Detailed Description_ section of [OpenCV: Camera Calibration and 3D Reconstruction](https://docs.opencv.org/3.4.3/d9/d0c/group__calib3d.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
