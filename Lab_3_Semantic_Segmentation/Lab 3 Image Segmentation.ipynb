{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Semantic Segmentation\n",
    "\n",
    "Welcome to the Semantic Segmentation lab! During this session, you'll gain insights into employing pretrained models for conducting segmentation on the Cityscapes dataset.\n",
    "<center>\n",
    "  <img src=\"img/segmentation.png\" style=\"width:700px;height:400;\">\n",
    "</center>\n",
    "<caption><center> <u><b> Figure 1 </u></b>: Semantic segmentation <br> </center></caption>\n",
    "In essence, our objective is to take an RGB color image and generate a segmentation mask. This mask will assign each pixel a class label, represented as an integer.\n",
    "    \n",
    "**In this assignment, you will:**\n",
    "- Implement function for the evaluation metrics\n",
    "- Preprocess raw images and segmentation mask\n",
    "- Make predictions with the pretrained models (FCN and Unet) and visualize segmentation masks.\n",
    "- Compare model performances. \n",
    "\n",
    "In most exercises, a suggested outline will be provided for you, and each exercise are expected to be solved within 15 lines.\n",
    "\n",
    "Please note that you are restricted to using the packages already loaded and the custom functions explained in the notebook. Run the cell below to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch dependency\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "# load image path\n",
    "image_path = glob('cityscapes_data/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Evaluation Metrics (IOU)\n",
    "Before we dive into the models for semantic segmentation, lets us first determine the evaluation metrics, specifically intersection over Union, which computes pixelwise agreement between the true mask and prediction: $$ IoU(X, X_{truth}) = \\frac{|X \\cap X_{truth}|}{|X \\cup X_{truth}|} = \\frac{|X \\cap X_{truth}|}{|X|+ |X_{truth}| - |X \\cap X_{truth}|} $$The following exercise requires you to design an evaluation metric function, and for simplicity, it only involves 3 classes (class 0, class 1, and class 2) and 10 by 10 images.   \n",
    "\n",
    "**Exerciseï¼š** Implement IOU function as an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(prediction, GT):\n",
    "    \"\"\" Intersection Over Unions\n",
    "    Arguments:\n",
    "        GT -- ground truth label, dimension: (Batch size, number of class, width, height)\n",
    "        prediction -- predicted segmentation, dimension: (Batch size, number of class, width, height)    \n",
    "    Returns:\n",
    "        IOU for the class\n",
    "    \"\"\"\n",
    "    eps = 1e-5 # remember to add a small number to denominator to avoid division by 0\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # Intersection term  |A ^ B|\n",
    "    inter = \n",
    "    \n",
    "    # sum of |A| + |B|\n",
    "    sets_sum = \n",
    "    \n",
    "    # IoU = |A ^ B| / |A \\/ B| = |A ^ B| / (|A| + |B| - |A^B|)\n",
    "    return (inter + eps) / (sets_sum - inter + eps)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests \n",
    "GT = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 1., 1., 1., 1., 1., 0.],\n",
    "                   [0., 0., 0., 1., 1., 1., 1., 0., 0., 0.],\n",
    "                   [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 0., 0., 0., 2., 2.],\n",
    "                   [0., 0., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "                   [0., 0., 2., 2., 2., 2., 2., 2., 2., 2.]]).long()\n",
    "\n",
    "Prediction = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 2., 1., 1., 1., 0., 0.],\n",
    "                   [0., 0., 0., 1., 1., 2., 1., 0., 0., 0.],\n",
    "                   [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 1., 1., 0., 0., 0., 2., 0.],\n",
    "                   [2., 2., 2., 1., 1., 1., 1., 2., 2., 0.],\n",
    "                   [2., 2., 2., 2., 2., 2., 2., 2., 0., 0.]]).long()\n",
    "\n",
    "score = IOU(F.one_hot(Prediction).unsqueeze(0).float(), F.one_hot(GT).unsqueeze(0).float()).item()\n",
    "assert np.isclose(0.7094017267227173, score), 'incorrect result!'\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data preprocessing\n",
    "### Cityscapes Dataset\n",
    "Cityscapes is a computer vision dataset for urban scene understanding. It includes high-resolution images from various cities in Europe, densely annotated with 30 semantic classes such as roads, buildings, cars, and pedestrians. It's widely used to develop and evaluate algorithms for autonomous vehicles and urban planning. We will its fine annotated samples, in which raw data and segmentation masks are concatenated in the same images. First, lets inspect a few samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(16, 8))\n",
    "for i in range(4):\n",
    "    # read the image\n",
    "    img = torchvision.io.read_image(image_path[i])\n",
    "\n",
    "    # pytorch reads it as (c, h, w), reshape it to (h, w, c) which is the shape matplotlib wants\n",
    "    img = img.permute(1, 2, 0)\n",
    "    \n",
    "    # calculate the indexes for plots and set the image data\n",
    "    y, x = i // 2, i % 2\n",
    "    axs[y, x].imshow(img.numpy())\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Unfortunately, raw images and segmentation masks are paired in RGB images, so we need split them and label masks. After preprocessing, the segmentation mask should have a shape of (256, 256). Additionally, to enhance inference speed, both the images and masks are resized to a more manageable size of (128, 128, 3) and (128, 128), respectively.\n",
    "\n",
    "**Exercise:** Preprocess the data\n",
    "1. Split raw images and masks\n",
    "2. label masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total classes name\n",
    "names = [ 'unlabeled','ego vehicle','rectification border', 'out of roi', 'static', 'dynamic','ground', 'road', 'sidewalk', 'parking', 'rail track', 'building', 'wall', 'fence','guard rail' , 'bridge','tunnel','pole', 'polegroup', 'traffic light', 'traffic sign' ,'vegetation', 'terrain', 'sky' ,'person', 'rider', 'car','truck' ,'bus', 'caravan','trailer', 'train' , 'motorcycle','bicycle','license plate']\n",
    "\n",
    "# colours in the segmentation mask\n",
    "idx_color = [[ 0,  0,  0], [ 0,  0,  0], [  0,  0,  0], [  0,  0,  0],[ 0,  0,  0],[111, 74,  0],[81,  0, 81] ,[128, 64,128],[244, 35,232],\n",
    "                [250,170,160],[230,150,140],[70, 70, 70],[102,102,156],[190,153,153],[180,165,180],[150,100,100],[150,120, 90],[153,153,153],\n",
    "                [153,153,153],[250,170, 30],[220,220,  0],[107,142, 35],[152,251,152],[ 70,130,180],[220, 20, 60],[255,  0,  0],[ 0,  0,142],\n",
    "                [ 0,  0, 70],[ 0, 60,100],[ 0,  0, 90],[  0,  0,110],[ 0, 80,100],[  0,  0,230],[119, 11, 32],[  0,  0,142]]\n",
    "\n",
    "\n",
    "idx_color_np = np.array(idx_color)\n",
    "# mappings, note that several classes might be mapped to the same number as we are not interested in unlabeled, ego vehicle, etc.\n",
    "mapping = {0 : 0, 1 : 0, 2 : 0, 3: 0, 4 : 0, 5 : 0, 6 : 0, 7 : 1, 8 : 1, 9 : 1, 10 : 1, 11 :2, 12 : 2, 13 : 2, 14 : 2, 15 : 2, 16 : 2,\n",
    "                    17 : 3, 18 : 3, 19 : 3, 20: 3, 21 : 4, 22 : 4, 23 : 5, 24 : 6, 25 : 6, 26 : 7, 27 : 7, 28 : 7, 29 : 7, 30 : 7, 31 : 7, 32: 7, 33 : 7, 34 : 7}\n",
    "\n",
    "from typing import Tuple\n",
    "# vectorize the operation of getting the class index to class number for numpy \n",
    "idx_to_category_mapping = lambda x: mapping[x]\n",
    "vectorized_cat_mapping = np.vectorize(idx_to_category_mapping)\n",
    "\n",
    "# vectorize the operation of mapping the class index to color for numpy \n",
    "idx_to_col_mapping = lambda x: idx_color[x]\n",
    "vectorized_col_mapping = np.vectorize(idx_to_col_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(path : str, downscale_factor=1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "        Read the .jpeg image from *path*. Return the input image (256 x 256 x 3), mask (256 x 256 x 3) read from the jpeg \n",
    "        and conversion to categories or class id representation (128 x 128)\n",
    "        Argmument: \n",
    "            path: the path to the image\n",
    "            downscale_factor: factor to resize the image\n",
    "        Return:\n",
    "            raw: resized raw image\n",
    "            mask: original mask in RGB\n",
    "            classes: new classes mask (single channel)\n",
    "    \"\"\"\n",
    "    # Read the image from path.\n",
    "    img = Image.open(path)\n",
    "    width, height = img.size\n",
    "    \n",
    "    if downscale_factor:\n",
    "        width, height = width // downscale_factor, height//downscale_factor \n",
    "        img = img.resize(( width, height))\n",
    "    \n",
    "    img = np.asarray(img)    \n",
    "    # then split the image into two images (in the middle of width) : input image and color mask (each represented by 3 channels)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    raw, mask = \n",
    "    height, width, channels = mask.shape\n",
    "\n",
    "    # compute then the sum of squared distances for each pixel to the colors (L2 between the color and pixel in mask) : \n",
    "    # the value which will be the minimal is the category name we will use for that pixel, and we will get it using argmin\n",
    "    distances = \n",
    "    classes = \n",
    "\n",
    "    # if we want to operate on names, map the categories to class number\n",
    "    classes = vectorized_cat_mapping(classes)\n",
    "    \n",
    "    return raw, mask, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mask_raw, classes = preprocess_image(image_path[0], downscale_factor=2)\n",
    "# sanity checks and print the data\n",
    "print(\"size of input : \", x.shape)\n",
    "print(\"size of mask raw : \", mask_raw.shape)\n",
    "print(\"size of classes : \", classes.shape)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('raw image')\n",
    "plt.imshow(x)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('original mask')\n",
    "plt.imshow(mask_raw)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('new mask')\n",
    "plt.imshow(classes, cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensor\n",
    "input_images = []\n",
    "masks = []\n",
    "# input image is normalized according to imagenet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "for path in image_path:\n",
    "    X, _, Y = preprocess_image(path, downscale_factor=2)\n",
    "    input_images.append(transform(torch.Tensor(X / 255.).permute(2, 0, 1)))\n",
    "    masks.append(torch.Tensor(Y))\n",
    "\n",
    "input_images = torch.stack(input_images)\n",
    "masks = torch.stack(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional Network (FCN) with naive upsampling\n",
    "<center>\n",
    "  <img src=\"img/FCN.png\" style=\"width:700px;height:400;\">\n",
    "</center>\n",
    "<caption><center> <u><b> Figure 2 </u></b>: Fully Convolutional Neural Networks Architecture  <br> </center></caption>\n",
    "\n",
    "Now we are ready to feed the model with the processed images. The actual architeture of the model is not exactly the same as Figure 2, so if you are interested in its implementation please check `model/NaiveNet.py`.\n",
    "\n",
    "**Exercise:**\n",
    "1. Make inference with FCN\n",
    "2. Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.NaiveNet import NaiveNet\n",
    "model_Naive = NaiveNet(n_channels=3, n_classes=len(set(mapping.values())))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#load weights from the pretrained model\n",
    "pretrained_weights = torch.load('models/naiveNet.pth', map_location=torch.device('cpu'))\n",
    "model_Naive.load_state_dict(pretrained_weights)\n",
    "model_Naive.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(batch, GT, predictions):\n",
    "    \"\"\" function to visualize input image, true masks and predicted masks\"\"\"\n",
    "    \n",
    "    batch_size = batch.shape[0]\n",
    "    fig, axes = plt.subplots(batch_size, 3, figsize=(6, 2.*batch_size), squeeze=True, sharey=True, sharex=True)\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img, mask = batch[i], GT[i]  \n",
    "        img = img.permute(1,2, 0) * Tensor([0.229, 0.224, 0.225]) + Tensor([0.485, 0.456, 0.406])\n",
    "        axes[i,0].imshow(img)\n",
    "        axes[i,0].set_xticks([])\n",
    "        axes[i,0].set_yticks([])\n",
    "        if i == 0:\n",
    "            axes[i, 0].set_title(\"Input Image\")\n",
    "\n",
    "        axes[i, 1].imshow(mask, cmap='magma')\n",
    "        axes[i,1].set_xticks([])\n",
    "        axes[i,1].set_yticks([])\n",
    "        if i == 0:\n",
    "            axes[i, 1].set_title(\"True Mask\")\n",
    "\n",
    "        predicted = predictions[i]\n",
    "        predicted = predicted.permute(1, 2, 0)\n",
    "        predicted = torch.argmax(predicted, dim=2)\n",
    "\n",
    "        axes[i, 2].imshow(predicted.cpu(), cmap='magma')\n",
    "        axes[i, 2].set_xticks([])\n",
    "        axes[i, 2].set_yticks([])\n",
    "        if i == 0:\n",
    "            axes[i, 2].set_title(\"Predicted Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "with torch.no_grad():\n",
    "    prediction_Naive = model_Naive(input_images.to(device))\n",
    "show_inference(input_images, masks, prediction_Naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Unet Model \n",
    "<center>\n",
    "  <img src=\"img/Unet.png\" style=\"width:600px;height:260;\">\n",
    "</center>\n",
    "<caption><center> <u><b> Figure 3 </u></b>: Fully Convolutional Neural Networks Architecture  <br> </center></caption>\n",
    "U-Net is a popular convolutional neural network (CNN) architecture designed for semantic image segmentation tasks. The U-Net architecture uses skip connections that pass feature maps from the contracting path to the corresponding layers in the expansive path. These skip connections help to retain high-resolution information, aiding in better localization of segmentation boundaries. \n",
    "If you like to know more about Unet, here is the link to the paper:\n",
    "https://arxiv.org/abs/1505.04597?ref=jeremyjordan.me\n",
    "    \n",
    "**Exercise:**\n",
    "1. Make inferences with Unet \n",
    "2. Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Unet import Unet\n",
    "# initilize the Unet\n",
    "model_Unet = Unet(n_channels=3, n_classes=len(set(mapping.values())))\n",
    "\n",
    "#load weights from the pretrained model\n",
    "pretrained_weights = torch.load('models/unet.pth', map_location=torch.device('cpu'))\n",
    "model_Unet.load_state_dict(pretrained_weights)\n",
    "model_Unet.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make inference\n",
    "with torch.no_grad():\n",
    "    prediction_Unet = model_Unet(input_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_inference(input_images, masks, prediction_Unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison \n",
    "As evident from the results, the FCN's performance is inferior to that of the Unet. However, to ensure a more rigorous comparison, we still require appropriate metrics.\n",
    "\n",
    "**Exercise:**\n",
    "1. Evaluate and compare the performance of two models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name, prediction, mask):\n",
    "    \"\"\"\n",
    "    evaluate model performance using cross entropy and IOU\n",
    "    Arguments:\n",
    "        model_name:\n",
    "        prediction: predicted segmentation\n",
    "        mask: groud truth mask\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    CE = criterion(prediction.to(device), masks.to(device).long())\n",
    "    # YOUR CODE STARTS HERE\n",
    "    iou = \n",
    "    print(f'{model_name}: \\n Cross entropy: {CE} \\n IOU: {iou} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('Unet', prediction_Unet, masks)\n",
    "evaluate('FCN', prediction_Naive, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe a lower cross-entropy for Unet, but a reduced IOU for FCN, well done, you've successfully completed this lab. you can checkout the references if you want to train a Unet from scratch. Unet is a very basic CNN model for semantic segmentation, while more advanced models currently are based on Visual transformers (Vit).If you are insterested in the latest models, here is a link to the leader board of the semantic segmention: https://paperswithcode.com/task/semantic-segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. Semantic segmentation post https://medium.com/@karansjc1/semantic-segmentation-using-pytorch-and-opencv-a98d8ddab228\n",
    "2. Traing Unet https://github.com/mlewandowski0/SemanticSegmentation\n",
    "3. Overview of Semantic Segmentation methods https://datahacker.rs/020-overview-of-semantic-segmentation-methods/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
